import torch
import sys, yaml, os

os.chdir(sys.path[0])
sys.path.append('../')
os.getcwd()

import argparse
from pprint import  pformat
import numpy as np
import torch
import time
import logging
from tqdm import tqdm
import random
from copy import deepcopy
from time import time
from torch.autograd import Variable

from torchvision.transforms import *
from utils.bd_dataset_v2 import get_labels
from utils.aggregate_block.save_path_generate import generate_save_folder
from utils.aggregate_block.dataset_and_transform_generate import get_num_classes, get_input_shape
from utils.aggregate_block.fix_random import fix_random
from utils.aggregate_block.dataset_and_transform_generate import dataset_and_transform_generate
from torch.utils.data import DataLoader, Subset, Dataset
from utils.aggregate_block.model_trainer_generate import generate_cls_model, generate_cls_trainer
from utils.aggregate_block.train_settings_generate import argparser_criterion, argparser_opt_scheduler


from pytorch_influence_functions.influence_function import I_pert_loss, avg_s_test, grad_z
from uba.uba_utils.basic_utils import *


class Attack(object):
    r"""
    Base class for all attacks.
    .. note::
        It automatically set device to the device where given model is.
        It temporarily changes the original model's training mode to `test`
        by `.eval()` only during an attack process.
    """

    def __init__(self, name, model, device):
        r"""
        Initializes internal attack state.
        Arguments:
            name (str) : name of an attack.
            model (torch.nn.Module): model to attack.
        """

        self.attack = name
        self.model = model
        self.model_name = str(model).split("(")[0]

        self.training = model.training
        self.device = device

        self._targeted = 1
        self._attack_mode = "original"
        self._return_type = "float"

    def forward(self, *input):
        r"""
        It defines the computation performed at every call.
        Should be overridden by all subclasses.
        """
        raise NotImplementedError

    def set_attack_mode(self, mode):
        r"""
        Set the attack mode.

        Arguments:
            mode (str) : 'original' (DEFAULT)
                         'targeted' - Use input labels as targeted labels.
                         'least_likely' - Use least likely labels as targeted labels.
        """
        if self._attack_mode == "only_original":
            raise ValueError(
                "Changing attack mode is not supported in this attack method."
            )

        if mode == ["original", "influence_adversarial"]:
            self._attack_mode = "original"
            self._targeted = 1 # gradient ascent for maximum 
            self._transform_label = self._get_label
        elif mode == "targeted":
            self._attack_mode = "targeted"
            self._targeted = -1
            self._transform_label = self._get_label
        elif mode == "least_likely":
            self._attack_mode = "least_likely"
            self._targeted = -1
            self._transform_label = self._get_least_likely_label
        else:
            raise ValueError(
                mode
                + " is not a valid mode. [Options : original, targeted, least_likely]"
            )

    def set_return_type(self, type):
        r"""
        Set the return type of adversarial images: `int` or `float`.
        Arguments:
            type (str) : 'float' or 'int'. (DEFAULT : 'float')
        """
        if type == "float":
            self._return_type = "float"
        elif type == "int":
            self._return_type = "int"
        else:
            raise ValueError(type + " is not a valid type. [Options : float, int]")

    def save(self, save_path, data_loader, verbose=True):
        r"""
        Save adversarial images as torch.tensor from given torch.utils.data.DataLoader.
        Arguments:
            save_path (str) : save_path.
            data_loader (torch.utils.data.DataLoader) : data loader.
            verbose (bool) : True for displaying detailed information. (DEFAULT : True)
        """
        self.model.eval()

        image_list = []
        label_list = []

        correct = 0
        total = 0

        total_batch = len(data_loader)

        for step, (images, labels) in enumerate(data_loader):
            adv_images = self.__call__(images, labels)

            image_list.append(adv_images.cpu())
            label_list.append(labels.cpu())

            if self._return_type == "int":
                adv_images = adv_images.float() / 255

            if verbose:
                outputs = self.model(adv_images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels.to(self.device)).sum()

                acc = 100 * float(correct) / total
                print(
                    "- Save Progress : %2.2f %% / Accuracy : %2.2f %%"
                    % ((step + 1) / total_batch * 100, acc),
                    end="\r",
                )

        x = torch.cat(image_list, 0)
        y = torch.cat(label_list, 0)
        torch.save((x, y), save_path)
        print("\n- Save Complete!")

        self._switch_model()

    def _transform_label(self, images, labels):
        r"""
        Function for changing the attack mode.
        """
        return labels

    def _get_label(self, images, labels):
        r"""
        Function for changing the attack mode.
        Return input labels.
        """
        return labels

    def _get_least_likely_label(self, images, labels):
        r"""
        Function for changing the attack mode.
        Return least likely labels.
        """
        outputs = self.model(images)
        _, labels = torch.min(outputs.data, 1)
        labels = labels.detach_()
        return labels

    def _to_uint(self, images):
        r"""
        Function for changing the return type.
        Return images as int.
        """
        return ((images * 0.5 + 0.5) * 255).type(torch.uint8)

    def _switch_model(self):
        r"""
        Function for changing the training mode of the model.
        """
        if self.training:
            self.model.train()
        else:
            self.model.eval()

    def __str__(self):
        info = self.__dict__.copy()

        del_keys = ["model", "attack"]

        for key in info.keys():
            if key[0] == "_":
                del_keys.append(key)

        for key in del_keys:
            del info[key]

        info["attack_mode"] = self._attack_mode
        if info["attack_mode"] == "only_original":
            info["attack_mode"] = "original"

        info["return_type"] = self._return_type

        return (
                self.attack
                + "("
                + ", ".join("{}={}".format(key, val) for key, val in info.items())
                + ")"
        )

    def __call__(self, *input, **kwargs):
        self.model.eval()
        images = self.forward(*input, **kwargs)
        self._switch_model()

        if self._return_type == "int":
            images = self._to_uint(images)

        return images

class Influence_Attack(Attack):
    r"""
    TODO: implement code note
    """

    def __init__(self, y_tgt,
                 model, 
                 s_vec,
                 img_size,
                 eps = 128 / 255, 
                 alpha = 4 / 255, 
                 random_start = False, 
                 device = None,
                 top_layers_num=None,
                 train_dataset_size=50000):
        super(Influence_Attack, self).__init__("Influence_Attack", model, device)
        self.eps = eps
        self.alpha = alpha
        self.random_start = random_start
        self.s_vec = s_vec
        self.y_tgt = y_tgt
        self.top_layers_num = top_layers_num
        self.img_size = img_size
        self.train_dataset_size = train_dataset_size
        
        if self.random_start:
            # Starting at a uniformly random point
            self.delta = torch.empty(self.img_size).uniform_(
                -self.eps, self.eps
            )
        else:
            self.delta = torch.zeros(self.img_size)
            
        self.delta = self.delta.to(self.device)
        
        self.total_influence = 0
        
    def _transform_label(self, images, labels):
        return torch.ones_like(labels) * self.y_tgt

    def forward(self, images, labels):
        r"""
        Overridden.
        """
        images = images.to(self.device)
        labels = self._transform_label(images, labels)
        labels = labels.to(self.device)
        
        net = self.model
        num_layers = len(list(net.children()))
        if self.top_layers_num is None:
            self.top_layers_num = num_layers
        
        self.delta = Variable(self.delta, requires_grad=True)
        adv_images = images + self.delta
        adv_images = torch.clamp(adv_images, min=-1, max=1)
        
        for idx, child in enumerate(net.children()):
            if idx < num_layers - self.top_layers_num:
                for param in child.parameters():
                    param.requires_grad = False
            else:
                for param in child.parameters():
                    param.requires_grad = True
                        
        influence =\
            calc_influence(net,
                    self.s_vec,
                    adv_images, labels,
                    self.device,
                    train_dataset_size=self.train_dataset_size)
        
        influence.requires_grad_(True)
        
        optim_target = influence - 0.01 * torch.norm(self.delta, p=float('inf'))
        
        optim_target.backward()
        
        grad = self.delta.grad
        
        self.delta = self.delta - self.alpha * grad
        self.delta = torch.clamp(self.delta, min=-self.eps, max=self.eps)      
        self.total_influence += float(influence.detach())

class Delta_Test_Dataset(Dataset):
    def __init__(self,
                 clean_dataset,
                 y_tgt,
                 delta) -> None:
        super().__init__()
        self.clean_dataset = clean_dataset
        self.y_tgt = y_tgt
        self.delta = delta.cpu()
        
    def __getitem__(self, index):
        image, label, *others = self.clean_dataset.__getitem__(index)
        image = torch.clamp(image + self.delta, min=-1, max=1)
        return image, self.y_tgt, others
    
    def __len__(self):
        return self.clean_dataset.__len__()

def set_args(parser):
    """
    parser : argparse.ArgumentParser
    return a parser added with args required by fit
    """
    # Training settings
    parser.add_argument('--amp', type=lambda x: str(x) in ['True', 'true', '1'])
    parser.add_argument('--device', type = str)
    parser.add_argument('--yaml_path', type=str, default="../config/influence/ibap/default.yaml",
                        help='path for yaml file provide additional default attributes')
    parser.add_argument('--lr_scheduler', type=str,
                        help='which lr_scheduler use for optimizer')
    parser.add_argument('--batch_size', type=int)
    parser.add_argument('--lr', type=float)
    parser.add_argument('--steplr_stepsize', type=int)
    parser.add_argument('--steplr_gamma', type=float)
    parser.add_argument('--sgd_momentum', type=float)
    parser.add_argument('--wd', type=float, help='weight decay of sgd')
    parser.add_argument('--steplr_milestones', type=list)
    parser.add_argument('--client_optimizer', type=int)
    parser.add_argument('--random_seed', type=int,
                        help='random_seed')
    parser.add_argument('--frequency_save', type=int,
                        help=' frequency_save, 0 is never')
    parser.add_argument('--save_folder_name', type=str,
                        help='(Optional) should be time str + given unique identification str')
    parser.add_argument('--result_folder', type=str, default=None)
    parser.add_argument("--result_name", type=str, default="attack_result.pt", 
                        help='the name of result file in the result folder')
    parser.add_argument("--mini_batch_size", type=int, default=1,
                        help='The mini batch size for hessian vector production.')
    parser.add_argument('--recursion_depth', type=int, default=5000,
                        help='the recursion depth, indicates the interation times')
    parser.add_argument('--r_averaging', type=int, default=10, help='repeate times for averaging')
    parser.add_argument('--scale', type=int, default=None, help='')
    parser.add_argument('--damp', type=int, default=None, help='')
    parser.add_argument('--attack', type=str)
    parser.add_argument("-n", "--num_workers", type=int, default=4, help="dataloader num_workers")
    parser.add_argument("-pm", "--pin_memory", type=lambda x: str(x) in ['True', 'true', '1'], default=True,
                        help="dataloader pin_memory")
    parser.add_argument("-nb", "--non_blocking", type=lambda x: str(x) in ['True', 'true', '1'], default=True,
                        help=".to(), set the non_blocking = ?")
    parser.add_argument("--subset_ratio", type=float, default=0.1, 
                        help='Choose subset_ratio * len(data) as a subset for backdoor construction, default is 0.1.') 
       
    parser.add_argument('--pgd_steps', type=int, default=40)
    parser.add_argument('--iteration_num', type=int, default=5)
    return parser

def add_yaml_to_args(args:argparse.ArgumentParser):
    with open(args.yaml_path, 'r') as f:
        clean_defaults = yaml.safe_load(f)
    clean_defaults.update({k: v for k, v in args.__dict__.items() if v is not None})
    args.__dict__ = clean_defaults

def process_args(args:argparse.ArgumentParser):
    args.terminal_info = sys.argv
    args.attack = 'None'
    args.surrogate_model_folder = args.result_folder
    args.dataset_folder = args.result_folder
    return args

def calc_influence(net,
                   s_test_avg, 
                   z, t,
                   device,
                   train_dataset_size):
    
    grad_z_vec = grad_z(z, t, net, device=device)
    # Correspoding code in release-function:
    # predicted_loss_diffs[counter] = np.dot(np.concatenate(inverse_hvp), np.concatenate(train_grad_loss_val)) / self.num_train_examples
    influence = sum(
        [
            torch.sum(k * j).data 
            for k, j in zip(grad_z_vec, s_test_avg)
        ]) / train_dataset_size
    
    return influence.cpu()

def get_dataset_per_class(clean_train_dataset_with_transform,
                        args):
    results = []
    clean_train_dataset_targets = get_labels(clean_train_dataset_with_transform)
    clean_train_dataset_targets = np.array(clean_train_dataset_targets)
    for i in range(args.num_classes):
        z_tgt_indices = np.argwhere(((clean_train_dataset_targets == i))).squeeze()
        z_other_indices = np.argwhere(((clean_train_dataset_targets != i))).squeeze()
        
        z_tgt_dataset = Subset(clean_train_dataset_with_transform, z_tgt_indices)
        
        sample_num = int(args.subset_ratio * len(z_other_indices)) 
        sample_list = random.sample(list(range(len(z_other_indices))), sample_num)
        z_other_indices = z_other_indices[sample_list]
        z_other_dataset = Subset(clean_train_dataset_with_transform, z_other_indices)
        
        results.append((z_tgt_dataset, z_other_dataset))

    return results




def main():

    ''' 
    1. config args, save_path, fix random seed 
    '''
    
    parser = argparse.ArgumentParser(description=sys.argv[0])
    parser = set_args(parser=parser)
    args = parser.parse_args()
    add_yaml_to_args(args=args)
    args = process_args(args)
    
    logging.info(f"get the training setting for specific dataset")
    

    ### save path
    if 'save_folder_name' not in args:
        save_path = generate_save_folder(
            run_info=('afterwards' if 'load_path' in args.__dict__ else 'attack') + '_' + args.attack,
            given_load_file_path=args.load_path if 'load_path' in args else None,
            all_record_folder_path='../record',
        )
    else:
        save_path = '../record/' + args.save_folder_name
        os.mkdir(save_path)

    args.save_path = save_path

    torch.save(args.__dict__, save_path + '/info.pickle')

    ### set the random seed
    fix_random(int(args.random_seed))

    ## set device
    device = torch.device(
        (
            f"cuda:{[int(i) for i in args.device[5:].split(',')][0]}" if "," in args.device else args.device
            # since DataParallel only allow .to("cuda")
        ) if torch.cuda.is_available() else "cpu"
    )
    
    ''' 
    2. get datasets and model
    '''
    args,\
    clean_train_dataset_with_transform_for_train, _,\
    clean_train_dataset_with_transform, clean_test_dataset_with_transform,\
    _, _ \
    = get_dataset(args)
    
    net = get_surrogate_model(args)
    
    dataset_results = get_dataset_per_class(clean_train_dataset_with_transform, args)
    
    z_dl = DataLoader(clean_train_dataset_with_transform, batch_size=8, shuffle=True, drop_last=False,
                    pin_memory=args.pin_memory, num_workers=args.num_workers, )
                
    '''
    3. test model performence 
    '''
    
    clean_test_dl = DataLoader(clean_test_dataset_with_transform, batch_size=args.batch_size, shuffle=False, drop_last=False,
                pin_memory=args.pin_memory, num_workers=args.num_workers, )
        
    _, clean_test_m, _ = \
        model_test(train_dl=None,
                   clean_test_dl=clean_test_dl,
                   bd_test_dl=None,
                   model=net,
                   args=args)
            
    logging.info(clean_test_m)
    test_acc = clean_test_m['test_correct']/clean_test_m['test_total']
    logging.info(f'test clean accuracy is {test_acc}')
    
    '''
    4. construct potential backdoor pattern for each class
    '''
    deltas = []
    for tgt, (z_tgt_dataset, z_other_dataset) in enumerate(dataset_results):
        logging.info('-'*10 + f' detect backdoor for class {tgt} ' + '-'*10)
        
        s_dataset = z_tgt_dataset
        s_dl = DataLoader(s_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False,
                    pin_memory=args.pin_memory, num_workers=args.num_workers, )
        s_vec = avg_s_test(test_loader=s_dl, 
                model=net, 
                z_loader=z_dl,
                device=device, 
                damp=args.damp, 
                scale=args.scale,
                recursion_depth=args.recursion_depth,
                )
        
        attacker = Influence_Attack(y_tgt = tgt, 
                                    model = net, 
                                    s_vec = s_vec,
                                    img_size = (args.input_channel, args.input_height, args.input_width),
                                    random_start=True,
                                    device=device,
                                    train_dataset_size=len(clean_train_dataset_with_transform)
                                    )
                
        z_other_dl = DataLoader(z_other_dataset, batch_size=1, shuffle=False, drop_last=False,
                    pin_memory=args.pin_memory, num_workers=args.num_workers, )
        
        for i in range(args.iteration_num):
            
            for j, (images, labels, *other_info) in enumerate(z_other_dl):
                attacker(images, labels)
                
                if j % 100 == 0:
                    delta_test_dataset = Delta_Test_Dataset(clean_test_dataset_with_transform, tgt, attacker.delta)
                    delta_test_dl = DataLoader(delta_test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False,
                                            pin_memory=args.pin_memory, num_workers=args.num_workers, )
                    _, _, bd_test_m = \
                        model_test(train_dl=None,
                                clean_test_dl=None,
                                bd_test_dl=delta_test_dl,
                                model=net,
                                args=args)
                                
                    asr = bd_test_m['test_correct']/bd_test_m['test_total']
                    norm = torch.norm(attacker.delta, p=float('inf'))
                    print(f'iteration {i}: the asr of class {tgt} is {asr}, and influence is {round(attacker.total_influence, 3)} with l_inf norm of {round(float(norm), 3)}\r', end='')
            
            attacker.total_influence = 0
        
        delta = attacker.delta
        deltas.append(delta)
        
        delta_test_dataset = Delta_Test_Dataset(clean_test_dataset_with_transform, tgt, delta)
        delta_test_dl = DataLoader(delta_test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False,
                pin_memory=args.pin_memory, num_workers=args.num_workers, )
        
        _, _, bd_test_m = \
            model_test(train_dl=None,
                    clean_test_dl=None,
                    bd_test_dl=delta_test_dl,
                    model=net,
                    args=args)
        
        print()
        logging.info(bd_test_m)
        asr = bd_test_m['test_correct']/bd_test_m['test_total']
        logging.info('-' * 10 + f'Eventually, the asr of class {tgt} is {asr}' + '-' * 10)
        
    NotImplemented
        
        

if __name__ == '__main__':
    main()
